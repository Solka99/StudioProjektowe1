{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-17T10:35:16.259875Z",
     "start_time": "2025-01-17T10:35:10.109940Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy import signal\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T10:35:17.362138Z",
     "start_time": "2025-01-17T10:35:16.294875Z"
    }
   },
   "cell_type": "code",
   "source": "data1 = pd.read_csv('data_scaled.csv')",
   "id": "ec42afb4970a6b80",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T10:37:24.691975Z",
     "start_time": "2025-01-17T10:37:24.673007Z"
    }
   },
   "cell_type": "code",
   "source": "data1.head()",
   "id": "b09d470b6de08634",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          x         y         z activity\n",
       "0  0.457376  0.689453  0.421540  Walking\n",
       "1  0.438482  0.717446  0.420169  Walking\n",
       "2  0.544191  0.755916  0.477840  Walking\n",
       "3  0.487291  0.800191  0.390204  Walking\n",
       "4  0.391350  0.836838  0.224368  Walking"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.457376</td>\n",
       "      <td>0.689453</td>\n",
       "      <td>0.421540</td>\n",
       "      <td>Walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.438482</td>\n",
       "      <td>0.717446</td>\n",
       "      <td>0.420169</td>\n",
       "      <td>Walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.544191</td>\n",
       "      <td>0.755916</td>\n",
       "      <td>0.477840</td>\n",
       "      <td>Walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.487291</td>\n",
       "      <td>0.800191</td>\n",
       "      <td>0.390204</td>\n",
       "      <td>Walking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.391350</td>\n",
       "      <td>0.836838</td>\n",
       "      <td>0.224368</td>\n",
       "      <td>Walking</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T10:37:25.159167Z",
     "start_time": "2025-01-17T10:37:25.130633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = data1[['x', 'y', 'z']]\n",
    "y = data1['activity']"
   ],
   "id": "32227105b0c46c4a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T10:37:25.499374Z",
     "start_time": "2025-01-17T10:37:25.428786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "list=data1.activity.unique()\n",
    "list"
   ],
   "id": "70bf12b20e3735bd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Walking', 'Jogging', 'Stairs', 'Sitting', 'Standing', 'Typing',\n",
       "       'Brushing Teeth', 'Eating Soup', 'Eating Chips', 'Eating Pasta',\n",
       "       'Drinking from Cup', 'Eating Sandwich', 'Kicking (Soccer Ball)',\n",
       "       'Playing Catch w/Tennis Ball', 'Dribbling (Basketball)', 'Writing',\n",
       "       'Clapping', 'Folding Clothes'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Podział na okna czasowe",
   "id": "f3240903e09a891"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T10:37:26.606158Z",
     "start_time": "2025-01-17T10:37:26.586055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Przygotowanie cech rolling\n",
    "# rolling_features = ['x_mean_rolling', 'y_mean_rolling', 'z_mean_rolling',\n",
    "#                     'x_std_rolling', 'y_std_rolling', 'z_std_rolling',\n",
    "#                     'x_range_rolling', 'y_range_rolling', 'z_range_rolling']\n",
    "#\n",
    "# # Ekstrakcja segmentów dla danych rolling\n",
    "# rolling_segments = []\n",
    "# for i in range(0, len(data) - N_TIME_STEPS, step):\n",
    "#     segment = data[rolling_features].iloc[i: i + N_TIME_STEPS].values.flatten()\n",
    "#     rolling_segments.append(segment)\n",
    "#\n",
    "# rolling_segments = np.asarray(rolling_segments)\n"
   ],
   "id": "2965d9bb758e86be",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T10:37:27.291704Z",
     "start_time": "2025-01-17T10:37:27.282708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from scipy.stats import mode\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import tensorflow as tf\n",
    "#\n",
    "# # Constants\n",
    "# N_TIME_STEPS = 100\n",
    "# N_FEATURES = 3\n",
    "# step = 30\n",
    "# RANDOM_SEED = 42\n",
    "#\n",
    "# segments = []\n",
    "# labels = []\n",
    "# from collections import Counter\n",
    "# # Loop to segment the data\n",
    "# for i in range(0, len(data1) - N_TIME_STEPS, step):\n",
    "#     xs = data1['x'].values[i: i + N_TIME_STEPS]\n",
    "#     ys = data1['y'].values[i: i + N_TIME_STEPS]\n",
    "#     zs = data1['z'].values[i: i + N_TIME_STEPS]\n",
    "#\n",
    "#     # Replace stats.mode with an alternative using collections.Counter\n",
    "#     segment_labels = data1['activity'][i: i + N_TIME_STEPS]\n",
    "#     label = Counter(segment_labels).most_common(1)[0][0]  # Calculate mode\n",
    "#\n",
    "#     # Append data and labels\n",
    "#     segments.append([xs, ys, zs])\n",
    "#     labels.append(label)\n",
    "#\n",
    "# # Reshape segments and encode labels\n",
    "# reshaped_segments = np.asarray(segments, dtype=np.float32).reshape(-1, N_TIME_STEPS, N_FEATURES)\n",
    "# labels = np.asarray(pd.get_dummies(labels), dtype=np.float32)\n",
    "#\n",
    "# # # Split the data\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(\n",
    "# #     reshaped_segments, labels, test_size=0.2, random_state=RANDOM_SEED)\n",
    "#\n",
    "# from sklearn.model_selection import train_test_split\n",
    "#\n",
    "# # Podział na dane treningowe i tymczasowe (walidacyjne + testowe)\n",
    "# X_train, X_temp, y_train, y_temp = train_test_split(reshaped_segments, labels, test_size=0.3, random_state=42)\n",
    "#\n",
    "# # Podział tymczasowych danych na walidacyjne i testowe\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "#\n",
    "# # Sprawdzenie rozmiarów\n",
    "# print(f\"Trening: {X_train.shape}, Walidacja: {X_val.shape}, Testy: {X_test.shape}\")\n",
    "#\n",
    "#\n",
    "# # Expand dimensions for TensorFlow compatibility\n",
    "# X_train = tf.expand_dims(X_train, axis=-1)\n",
    "# X_test = tf.expand_dims(X_test, axis=-1)\n",
    "# X_val = tf.expand_dims(X_val, axis=-1)"
   ],
   "id": "6f6b40b8027cefef",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T10:50:57.071583Z",
     "start_time": "2025-01-17T10:50:50.621972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# Constants\n",
    "N_TIME_STEPS = 100\n",
    "N_FEATURES = 3  # Początkowo 3 cechy: x, y, z\n",
    "step = 30\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "segments = []\n",
    "labels = []\n",
    "\n",
    "# Loop to segment the data\n",
    "for i in range(0, len(data1) - N_TIME_STEPS, step):\n",
    "    xs = data1['x'].values[i: i + N_TIME_STEPS]\n",
    "    ys = data1['y'].values[i: i + N_TIME_STEPS]\n",
    "    zs = data1['z'].values[i: i + N_TIME_STEPS]\n",
    "\n",
    "    # Średnia w czasie (dla każdej cechy w segmencie)\n",
    "    mean_x = np.mean(xs)\n",
    "    mean_y = np.mean(ys)\n",
    "    mean_z = np.mean(zs)\n",
    "\n",
    "    # Tworzenie segmentu\n",
    "    segment = np.column_stack((xs, ys, zs))  # Oryginalne dane czasowe\n",
    "\n",
    "    # Dodanie średnich jako dodatkowych cech (po czasie)\n",
    "    extra_features = [mean_x, mean_y, mean_z]\n",
    "    segment = np.hstack((segment, np.tile(extra_features, (N_TIME_STEPS, 1))))  # Powielanie cech na każdym kroku czasowym\n",
    "\n",
    "    # Najczęściej występująca etykieta\n",
    "    segment_labels = data1['activity'][i: i + N_TIME_STEPS]\n",
    "    label = Counter(segment_labels).most_common(1)[0][0]\n",
    "\n",
    "    # Append segment and label\n",
    "    segments.append(segment)\n",
    "    labels.append(label)\n",
    "\n",
    "# Przekształć dane w odpowiedni format\n",
    "reshaped_segments = np.asarray(segments, dtype=np.float32)  # (n_samples, N_TIME_STEPS, N_FEATURES + extra_features)\n",
    "labels = np.asarray(pd.get_dummies(labels), dtype=np.float32)\n",
    "\n",
    "# Podział na dane treningowe i tymczasowe (walidacyjne + testowe)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(reshaped_segments, labels, test_size=0.3, random_state=RANDOM_SEED)\n",
    "\n",
    "# Podział tymczasowych danych na walidacyjne i testowe\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=RANDOM_SEED)\n",
    "\n",
    "# Sprawdzenie rozmiarów\n",
    "print(f\"Trening: {X_train.shape}, Walidacja: {X_val.shape}, Testy: {X_test.shape}\")\n",
    "\n",
    "# Expand dimensions for TensorFlow compatibility (jeśli wymagane przez model)\n",
    "X_train = tf.expand_dims(X_train, axis=-1)\n",
    "X_test = tf.expand_dims(X_test, axis=-1)\n",
    "X_val = tf.expand_dims(X_val, axis=-1)\n"
   ],
   "id": "b9165f862e1afe6f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trening: (15129, 100, 6), Walidacja: (3242, 100, 6), Testy: (3242, 100, 6)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T10:51:04.838440Z",
     "start_time": "2025-01-17T10:51:04.577356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(filters=64, kernel_size=5, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(filters=128, kernel_size=7, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(filters=256, kernel_size=7, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(18, activation='softmax')\n",
    "])\n",
    "\n",
    "# Definicja modelu\n",
    "# model = Sequential([\n",
    "#     Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "#     # Pierwsza warstwa konwolucyjna\n",
    "#     Conv1D(filters=32, kernel_size=5, activation='relu'),\n",
    "#     MaxPooling1D(pool_size=2),\n",
    "#\n",
    "#     # Druga warstwa konwolucyjna\n",
    "#     Conv1D(filters=64, kernel_size=5, activation='relu'),\n",
    "#     MaxPooling1D(pool_size=2),\n",
    "#\n",
    "#     # Trzecia warstwa konwolucyjna\n",
    "#     Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "#     MaxPooling1D(pool_size=2),\n",
    "#\n",
    "#     # Spłaszczenie danych\n",
    "#     Flatten(),\n",
    "#\n",
    "#     # Warstwy Dense\n",
    "#     Dense(128, activation='relu'),\n",
    "#     Dropout(0.5),  # Zapobieganie przeuczeniu\n",
    "#     Dense(64, activation='relu'),\n",
    "#\n",
    "#     # Warstwa wyjściowa\n",
    "#     Dense(18, activation='softmax')\n",
    "# ])\n",
    "# model = Sequential([\n",
    "#     Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "#     # Pierwsza warstwa konwolucyjna\n",
    "#     Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'),\n",
    "#     MaxPooling1D(pool_size=2),\n",
    "#\n",
    "#     # Druga warstwa konwolucyjna\n",
    "#     Conv1D(filters=64, kernel_size=5, activation='relu', padding='same'),\n",
    "#     MaxPooling1D(pool_size=2),\n",
    "#\n",
    "#     # Trzecia warstwa konwolucyjna\n",
    "#     Conv1D(filters=128, kernel_size=5, activation='relu', padding='same'),\n",
    "#     MaxPooling1D(pool_size=2),\n",
    "#\n",
    "#     # Spłaszczenie danych\n",
    "#     Flatten(),\n",
    "#\n",
    "#     # Warstwy Dense\n",
    "#     Dense(128, activation='relu'),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(64, activation='relu'),\n",
    "#\n",
    "#     # Warstwa wyjściowa\n",
    "#     Dense(18, activation='softmax')\n",
    "# ])\n"
   ],
   "id": "62decec18f36c91b",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T10:51:06.026780Z",
     "start_time": "2025-01-17T10:51:05.986018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Kompilacja modelu\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ],
   "id": "c511459f0549d2a5",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-17T10:51:11.617045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('cnn_best_model3.keras', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                    epochs=100, batch_size=32, verbose=2, callbacks=[es, mc])\n"
   ],
   "id": "55582d8f5bd75d67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.25632, saving model to cnn_best_model3.keras\n",
      "473/473 - 18s - 39ms/step - accuracy: 0.3154 - loss: 1.9893 - val_accuracy: 0.2563 - val_loss: 2.3735\n",
      "Epoch 2/100\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T02:14:26.610313Z",
     "start_time": "2025-01-17T02:14:26.279406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Wczytaj najlepszy model\n",
    "best_model = load_model('cnn_best_model.keras')\n"
   ],
   "id": "f6764bbf8ed91090",
   "outputs": [],
   "execution_count": 189
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T02:14:29.260472Z",
     "start_time": "2025-01-17T02:14:28.112880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ewaluacja modelu\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(f\"Dokładność na zbiorze testowym: {accuracy:.2f}\")\n"
   ],
   "id": "2ea4f3d2d4d5d807",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m102/102\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 5ms/step - accuracy: 0.8971 - loss: 0.3097\n",
      "Dokładność na zbiorze testowym: 0.90\n"
     ]
    }
   ],
   "execution_count": 190
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8004f5972ef4ac43"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
